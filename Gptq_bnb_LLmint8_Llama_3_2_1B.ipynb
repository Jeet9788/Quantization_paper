{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iot02hTCwxVP"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate optimum transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install auto-gptq"
      ],
      "metadata": {
        "id": "V-4Hm2rZxE7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bitsandbytes>=0.39.0\n"
      ],
      "metadata": {
        "id": "hZ2VZUkRK33h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM , GPTQConfig , BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=\"Your_token_from_hugging_face\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "orig_model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
        "\n",
        "\n",
        "# quantization_8_config = GPTQConfig(\n",
        "#      bits=8,\n",
        "#      group_size=128,\n",
        "#      dataset=\"wikitext2\",\n",
        "#      desc_act=False,\n",
        "# )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# quant8_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_8_config, device_map='auto')\n",
        "\n",
        "# quantization_4_config = GPTQConfig(\n",
        "#      bits=4,\n",
        "#      group_size=128,\n",
        "#      dataset=\"wikitext2\",\n",
        "#      desc_act=False,\n",
        "# )\n",
        "\n",
        "# quant4_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_4_config, device_map='auto')\n",
        "\n",
        "# quantization_3_config = GPTQConfig(\n",
        "#      bits=3,\n",
        "#      group_size=128,\n",
        "#      dataset=\"wikitext2\",\n",
        "#      desc_act=False,\n",
        "# )\n",
        "\n",
        "# quant3_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_3_config, device_map='auto')\n",
        "\n",
        "\n",
        "# quantization_2_config = GPTQConfig(\n",
        "#      bits=2,\n",
        "#      group_size=128,\n",
        "#      dataset=\"wikitext2\",\n",
        "#      desc_act=False,\n",
        "# )\n",
        "\n",
        "# quant2_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_2_config, device_map='auto')\n",
        "\n",
        "# model_int8 = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "#                                              device_map='auto',\n",
        "#                                              load_in_8bit=True,\n",
        "#                                              )\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
        "# )\n",
        "\n",
        "# bnb4_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
        "\n"
      ],
      "metadata": {
        "id": "uXj6ZE4axE-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"gptq4 size: {orig_model.get_memory_footprint():,} bytes\")\n"
      ],
      "metadata": {
        "id": "Gd1y_vOqxFBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, input_text, max_length=50):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    output = model.generate(inputs=input_ids,\n",
        "                            max_length=max_length,\n",
        "                            do_sample=True,\n",
        "                            top_k=30,\n",
        "                            pad_token_id=tokenizer.eos_token_id,\n",
        "                            attention_mask=input_ids.new_ones(input_ids.shape))\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "def calculate_perplexity(model, text):\n",
        "    # Encode the text\n",
        "    encodings = tokenizer(text, return_tensors='pt').to(device)\n",
        "\n",
        "    # Define input_ids and target_ids\n",
        "    input_ids = encodings.input_ids\n",
        "    target_ids = input_ids.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "\n",
        "    # Loss calculation\n",
        "    neg_log_likelihood = outputs.loss\n",
        "\n",
        "    # Perplexity calculation\n",
        "    ppl = torch.exp(neg_log_likelihood)\n",
        "\n",
        "    return ppl"
      ],
      "metadata": {
        "id": "HZwKQMsVxFE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text4 = generate_text(quant4_model, \"I have a dream\")\n",
        "print(f\" model:\\n{text4}\")"
      ],
      "metadata": {
        "id": "l5YdCbj3xikW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb4_perplexity = calculate_perplexity(bnb4_model, text_bnb4)\n",
        "print(f\"bnb4_perplexity (model): {bnb4_perplexity.item():.2f}\")"
      ],
      "metadata": {
        "id": "jxew227qxjUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(orig_model)"
      ],
      "metadata": {
        "id": "Gwr1IA4NxoZ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}