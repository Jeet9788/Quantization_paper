{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfUXcCFnpi3DzDZmegbbb6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeet9788/Quantization_paper/blob/main/bnb_nf4_gpt-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxUBnJXllbdc",
        "outputId": "8cc33cf0-1380-4451-f2d6-ccf35c6a1421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_id = \"gpt2\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "quant_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhN8HnympRW4",
        "outputId": "ae2785ab-4667-4454-f8a8-3068ea70e82d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model size: {quant_model.get_memory_footprint():,} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrkZ7a9npjFD",
        "outputId": "d73a2a2a-47bf-496a-bada-62feb83bb48c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model size: 134,060,568 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n"
      ],
      "metadata": {
        "id": "AS_2M-3PMq-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "G5L4SjM4NoQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "U_1as8wFNrXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = quant_model.config.n_positions\n",
        "stride = 512\n",
        "seq_len = encodings.input_ids.size(1)\n",
        "\n",
        "nll_sum = 0.0\n",
        "n_tokens = 0\n",
        "prev_end_loc = 0\n",
        "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
        "    end_loc = min(begin_loc + max_length, seq_len)\n",
        "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
        "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
        "    target_ids = input_ids.clone()\n",
        "    target_ids[:, :-trg_len] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = quant_model(input_ids, labels=target_ids)\n",
        "\n",
        "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
        "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
        "        # to the left by 1.\n",
        "        neg_log_likelihood = outputs.loss\n",
        "\n",
        "    # Accumulate the total negative log-likelihood and the total number of tokens\n",
        "    num_valid_tokens = (target_ids != -100).sum().item()  # number of valid tokens in target_ids\n",
        "    batch_size = target_ids.size(0)\n",
        "    num_loss_tokens = num_valid_tokens - batch_size  # subtract batch_size due to internal label shift\n",
        "    nll_sum += neg_log_likelihood * num_loss_tokens\n",
        "    n_tokens += num_loss_tokens\n",
        "\n",
        "    prev_end_loc = end_loc\n",
        "    if end_loc == seq_len:\n",
        "        break\n",
        "\n",
        "avg_nll = nll_sum / n_tokens  # average negative log-likelihood per token\n",
        "ppl = torch.exp(avg_nll)\n",
        "print(f\"Avg NLL: {avg_nll:.4f}, Total tokens: {n_tokens}, Perplexity: {ppl:.2f}\")"
      ],
      "metadata": {
        "id": "dzDHlNsENwId"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}