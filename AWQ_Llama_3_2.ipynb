{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate optimum transformers\n",
        "!pip install autoawq"
      ],
      "metadata": {
        "id": "ZVtz0igDfcWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from awq import AutoAWQForCausalLM\n",
        "import torch\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=\"your_hugging_face_token\")\n"
      ],
      "metadata": {
        "id": "nljLQ5gIfeRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"meta-llama/Llama-3.2-1B\"\n",
        "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\":4}\n"
      ],
      "metadata": {
        "id": "MNwSz32_bweL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoAWQForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)"
      ],
      "metadata": {
        "id": "WoDP0IlsfsKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.quantize(tokenizer, quant_config=quant_config)\n"
      ],
      "metadata": {
        "id": "0jOQXLVqfwDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_path = \"LLama-3.2-awq\"\n"
      ],
      "metadata": {
        "id": "igUjhArMfwov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AwqConfig, AutoConfig\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "# modify the config file so that it is compatible with transformers integration\n",
        "quantization_config = AwqConfig(\n",
        "    bits=quant_config[\"w_bit\"],\n",
        "    group_size=quant_config[\"q_group_size\"],\n",
        "    zero_point=quant_config[\"zero_point\"],\n",
        ").to_dict()\n",
        "\n",
        "# the pretrained transformers model is stored in the model attribute + we need to pass a dict\n",
        "model.model.config.quantization_config = quantization_config\n",
        "# a second solution would be to use Autoconfig and push to hub (what we do at llm-awq)\n",
        "\n",
        "\n",
        "# save model weights\n",
        "model.save_quantized(quant_path)\n",
        "tokenizer.save_pretrained(quant_path)"
      ],
      "metadata": {
        "id": "v7iOT4FDlLO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! huggingface-cli login"
      ],
      "metadata": {
        "id": "EyQ_z48WlXoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "username = \"your_username\"\n",
        "MODEL_NAME = quant_path\n",
        "api = HfApi(token=\"your_hugging_face_token\")\n",
        "\n",
        "\n",
        "api.create_repo(\n",
        "    repo_id = f\"{username}/{MODEL_NAME}\",\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "\n",
        "api.upload_folder(\n",
        "    repo_id = f\"{username}/{MODEL_NAME}\",\n",
        "    folder_path=\"/content/LLama-3.2-awq\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "4osFLCC_mCeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{username}/LLama-3.2-awq\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"{username}/LLama-3.2-awq\").to(0)"
      ],
      "metadata": {
        "id": "EJLzvpNXn7Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"size: {model.get_memory_footprint():,} bytes\")\n"
      ],
      "metadata": {
        "id": "Mi8tafBToKa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "MnxDS9RWzEDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, input_text, max_length=50):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
        "    output = model.generate(inputs=input_ids,\n",
        "                            max_length=max_length,\n",
        "                            do_sample=True,\n",
        "                            top_k=30,\n",
        "                            pad_token_id=tokenizer.eos_token_id,\n",
        "                            attention_mask=input_ids.new_ones(input_ids.shape))\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "def calculate_perplexity(model, text):\n",
        "    # Encode the text\n",
        "    encodings = tokenizer(text, return_tensors='pt').to(device)\n",
        "\n",
        "    # Define input_ids and target_ids\n",
        "    input_ids = encodings.input_ids\n",
        "    target_ids = input_ids.clone()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "\n",
        "    # Loss calculation\n",
        "    neg_log_likelihood = outputs.loss\n",
        "\n",
        "    # Perplexity calculation\n",
        "    ppl = torch.exp(neg_log_likelihood)\n",
        "\n",
        "    return ppl"
      ],
      "metadata": {
        "id": "5bXXRBp3zY9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = generate_text(model, \"I have a dream\")\n",
        "print(f\" model:\\n{text}\")"
      ],
      "metadata": {
        "id": "tb2M-HmSzhZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity = calculate_perplexity(model, text)\n",
        "print(f\"perplexity (model): {perplexity.item():.2f}\")"
      ],
      "metadata": {
        "id": "U5311yGAzk0I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}